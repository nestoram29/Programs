{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65cdfe93",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16dfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a08f35",
   "metadata": {},
   "source": [
    "### Set up Tokenizer\n",
    "This object is used to convert sentences from a string to a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d937a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the tokenizer from DistilRoBERTa\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1ab18",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "These functions are used to load and transform the data into the appropriate format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    \"\"\"Converts the text of each example to \"input_ids\", a sequence of integers\n",
    "    representing 1-hot vectors for each token in the text\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation = True,\n",
    "        max_length = 64,\n",
    "        padding = \"max_length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de91fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padToZero(example):\n",
    "    \"\"\"Embedding layer expects mask value to be zero, but the tokenizer's\n",
    "    pad value is 1. This function changes 0s to 1s and 1s to 0s\"\"\"\n",
    "    vector = np.array(example[\"input_ids\"])\n",
    "    return {\n",
    "        \"input_ids\": np.where((vector == 0) | (vector == 1), vector^1, vector)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79e14a",
   "metadata": {},
   "source": [
    "tokenize() and padToZero() are used in the following function.\n",
    "This function is used by both train() and search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35203795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_path, dev_path):\n",
    "    # load the CSVs into Huggingface datasets to allow use of the tokenizer\n",
    "    hf_dataset = datasets.load_dataset(\"csv\", data_files={\n",
    "        \"train\": train_path, \"validation\": dev_path})\n",
    "\n",
    "    # the labels are the names of all columns except the first\n",
    "    labels = hf_dataset[\"train\"].column_names[1:]\n",
    "\n",
    "    def gather_labels(example):\n",
    "        \"\"\"Converts the label columns into a list of 0s and 1s\"\"\"\n",
    "        # the float here is because F1Score requires floats\n",
    "        return {\"labels\": [float(example[l]) for l in labels]}\n",
    "\n",
    "    # convert text and labels to format expected by model\n",
    "    hf_dataset = hf_dataset.map(gather_labels)\n",
    "    hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
    "    hf_dataset = hf_dataset.map(padToZero)\n",
    "\n",
    "    # convert Huggingface datasets to Tensorflow datasets\n",
    "    train_dataset = hf_dataset[\"train\"].to_tf_dataset(\n",
    "        columns = 'input_ids',\n",
    "        label_cols = \"labels\",\n",
    "        batch_size = 8,\n",
    "        shuffle = True,\n",
    "    )\n",
    "\n",
    "    dev_dataset = hf_dataset[\"validation\"].to_tf_dataset(\n",
    "        columns = 'input_ids',\n",
    "        label_cols = \"labels\",\n",
    "        batch_size = 8\n",
    "    )\n",
    "\n",
    "    return train_dataset, dev_dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd3403",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "The final model I ended up with is a dual bidrectional RNN with GRU units.\n",
    "The first layer is an embedding layer that transforms each integer in the input to a vector of size 256. \n",
    "`mask_zero = True` is there to make sure that the padding is ignored. \n",
    "The embedding layer is followed by the first bidirectionalo RNN layer. This layer returns a sequence.\n",
    "This is followed by the second bidirectional RNN layer. I initially used LSTM units but I could not get the dev score to get above 0.80.\n",
    "I tried reducing the complexity of the model but in the end using GRUs helped breaking through the 0.80 score. \n",
    "Having a dropout > 0.5 was also crucial for this.\n",
    "Finally, the last layer is the output layer which is used for multi-class non-exclusive classification.\n",
    "To allow for non-exclusivity, the sigmoid activation function was used (along with the proper loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_path = \"model.keras\", train_path = \"train.csv\", dev_path = \"dev.csv\"):\n",
    "    train_dataset, dev_dataset, labels = prepare_data(train_path, dev_path)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim = tokenizer.vocab_size + 1,\n",
    "                output_dim = 256,\n",
    "                mask_zero = True,\n",
    "            ),\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.GRU(\n",
    "                    units = 256,\n",
    "                    return_sequences = True,\n",
    "                    ),\n",
    "            ),\n",
    "            tf.keras.layers.Dropout(0.56),\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.GRU(\n",
    "                    units = 160\n",
    "                ),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                units = len(labels),\n",
    "                activation = 'sigmoid'\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    # specify compilation hyperparameters\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0006),\n",
    "        loss = tf.keras.losses.binary_focal_crossentropy,\n",
    "        metrics = [tf.keras.metrics.F1Score(average = \"micro\", threshold = 0.5)])\n",
    "\n",
    "    checkpointCallback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = model_path,\n",
    "        monitor = \"val_f1_score\",\n",
    "        mode = \"max\",\n",
    "        save_best_only = True\n",
    "    )\n",
    "\n",
    "    earlyStopCallback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_f1_score',\n",
    "        mode = 'max',\n",
    "        patience = 5,\n",
    "    )\n",
    "\n",
    "    # fit the model to the training data, monitoring F1 on the dev data\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs = 50,\n",
    "        validation_data = dev_dataset,\n",
    "        callbacks = [\n",
    "            checkpointCallback,\n",
    "            earlyStopCallback\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab137091",
   "metadata": {},
   "source": [
    "If I had more time I would have kept tuning the learning rate. \n",
    "This was very important to getting a decent generalization score, and with more time could have resulted in a better score.\n",
    "I also think the model could have benefited from a learning rate schedule since the model kept getting its best dev score in 2 or 3 epochs of training and then would drop after while the training score kept improving. I know that the typical \"fix\" for overfitting is to reduce the model complexity but somehow it seemed that I could not reach (let alone break) a dev score of .80 without making the model more complex. I tried adding as much dropout as I could initially but I could not break a dev score of 0.78. Until this current iteration of the model did I obtain 0.80. Further tuning allowed reaching 0.83.\n",
    "\n",
    "I ended up using binary_focal_crossentropy as the loss function in hopes of improving the dev score. I forgot to switch back binary_crossentropy but it did not seem to hurt. From my understanding, binary_focal_crossentropy focuses on (weights more) the more difficult samples.\n",
    "\n",
    "Two callbacks are used. ModelCheckpoint is used to save the model with the best validation score.\n",
    "EarlyStopping is used to stop training if the validation score does not see improvement.\n",
    "With the current iteration of the model, I found the best value for patience was 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4139ddb",
   "metadata": {},
   "source": [
    "### Hyperparameter Search\n",
    "I went through several iterations of this function. \n",
    "Its current state is what I had after getting my best dev score (though it is basically what I had to get me to my final model architecture).\n",
    "Random search was to try in an attempt to find the best hyperparameters. I was hoping once I narrowed it down I would switch to a grid search but I never got to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f94a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(train_path = \"train.csv\", dev_path = \"dev.csv\"):\n",
    "    train_dataset, dev_dataset, labels = prepare_data(train_path, dev_path)\n",
    "\n",
    "    def build_model(hp):\n",
    "        embedding_size = hp.Int(\n",
    "            'emb_size',\n",
    "            min_value = 64,\n",
    "            max_value = 256,\n",
    "            step = 32,\n",
    "        )\n",
    "        bilstm_0 = hp.Int(\n",
    "            'bilstm_0',\n",
    "            min_value = 64,\n",
    "            max_value = 256,\n",
    "            step = 32,\n",
    "        )\n",
    "        bilstm_1 = hp.Int(\n",
    "            'bilstm_1',\n",
    "            min_value = 64,\n",
    "            max_value = 256,\n",
    "            step = 32,\n",
    "        )\n",
    "        dropout = hp.Float(\n",
    "            'dropout_0',\n",
    "            min_value = 0.5,\n",
    "            max_value = 0.64,\n",
    "            step = 0.02,\n",
    "        )\n",
    "        learning_rate_0 = hp.Float(\n",
    "            'learning_rate',\n",
    "            min_value = 0.000001,\n",
    "            max_value = 0.0005,\n",
    "            step = 0.000005\n",
    "        )\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim = tokenizer.vocab_size + 1,\n",
    "                output_dim = embedding_size,\n",
    "                mask_zero = True,\n",
    "            ),\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.GRU(\n",
    "                    units = bilstm_0,\n",
    "                    return_sequences = True,\n",
    "                    ),\n",
    "            ),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.GRU(\n",
    "                    units = bilstm_1\n",
    "                ),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                units = len(labels),\n",
    "                activation = 'sigmoid'\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        # specify compilation hyperparameters\n",
    "        model.compile(\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate_0),\n",
    "            loss = tf.keras.losses.binary_focal_crossentropy,\n",
    "            metrics = [tf.keras.metrics.F1Score(average = \"micro\", threshold = 0.5)]\n",
    "        )\n",
    "    \n",
    "        return model\n",
    "\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        build_model,\n",
    "        objective = keras_tuner.Objective('val_f1_score', direction = 'max'),\n",
    "        max_trials = 100,\n",
    "        executions_per_trial = 1,\n",
    "        directory = '.',\n",
    "        project_name = 'RandomSearch_7',\n",
    "        overwrite = False,\n",
    "    )\n",
    "\n",
    "    earlyStopCallback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_f1_score',\n",
    "        mode = 'max',\n",
    "        patience = 4,\n",
    "    )\n",
    " \n",
    "    tuner.search(\n",
    "        train_dataset,\n",
    "        validation_data = dev_dataset,\n",
    "        epochs = 50,\n",
    "        callbacks = [earlyStopCallback]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1fe61",
   "metadata": {},
   "source": [
    "### Predict\n",
    "This function is used to load in a .csv file, prepare the data, and classify the sentences in the data using the model generated by train()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87469708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_path = \"model.keras\", input_path = \"test-ref.csv\"):\n",
    "    # load the saved model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # load the data for prediction\n",
    "    # use Pandas here to make assigning labels easier later\n",
    "    df = pandas.read_csv(input_path)\n",
    "\n",
    "    # create input features in the same way as in train()\n",
    "    hf_dataset = datasets.Dataset.from_pandas(df)\n",
    "    hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
    "    hf_dataset = hf_dataset.map(padToZero)\n",
    "    tf_dataset = hf_dataset.to_tf_dataset(\n",
    "        columns = 'input_ids',\n",
    "        batch_size = 8,\n",
    "    )\n",
    "\n",
    "    # generate predictions from model\n",
    "    predictions = np.where(model.predict(tf_dataset) > 0.5, 1, 0)\n",
    "\n",
    "    # assign predictions to label columns in Pandas data frame\n",
    "    df.iloc[:, 1:] = predictions\n",
    "\n",
    "    # write the Pandas dataframe to a zipped CSV file\n",
    "    df.to_csv(\"submission.zip\", index=False, compression=dict(\n",
    "        method='zip', archive_name=f'submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550752a5",
   "metadata": {},
   "source": [
    "## Train & Predict\n",
    "The following cell expects the \"train.csv\" and \"dev.csv\" to exist in the same directory that the notebook is executed from.\n",
    "The files paths can be changed by updating the path arguments.\n",
    "train() will generate a Keras model in the specified path. This file will then be used by predict() which loads in said model to do predictions on the file specified by the `input_path` argument. All input files are expected to be `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_path = \"model.keras\"\n",
    "\n",
    "train(model_path = mode_path, train_path = \"train.csv\", dev_path = \"dev.csv\")\n",
    "\n",
    "predict(model_path = mode_path, input_path = \"test-ref.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
